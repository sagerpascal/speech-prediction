mode: eval # either train or eval
use_wandb: False

# Environment
env:
  world_size: 1 # number of GPUs to run a batch in parallel

# Mask MFCC
masking:
  position: end # end, beginning or center
  n_frames: 120 # max. number of input frames
  k_frames: 30 # number of frames to predict
  window_shift: None # None = n_frames+k_frames
  start_idx: sliding-window # beginning (first n+k frames affected per file), random (n+k frames at random position per file), sliding-window (first n+k frames, then shift by n+k -> multiple frames per file)

# Dataset
data:
  config_file: datasets/timit-original.yaml

# Model settings
model:
  type: transformer # either unet or transformer
  transformer:
    n_heads: 8
    n_encoder_layers: 6
    n_decoder_layers: 6


# Training settings
train:
  max_number_of_epochs: 100
  early_stopping: False

# LR Scheduler
lr_scheduler:
  activate: True

# Optimizer
optimizer:
  type: adam